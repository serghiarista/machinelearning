{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e499580f",
   "metadata": {},
   "source": [
    "**Lista de Ejercicios 3** \n",
    "\n",
    "\n",
    "Grupo:\n",
    "1. Mayra Gavilán (20131530)\n",
    "2. Milagros Meza (20120186)\n",
    "3. Paola Cordova (20130365)\n",
    "4. Serghi Arista (20125341)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd5acf",
   "metadata": {},
   "source": [
    "**Pregunta 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffeedef",
   "metadata": {},
   "source": [
    "Brinde una explicación detallada del algoritmo que se utiliza para implementar un Árbol de Regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03de0c",
   "metadata": {},
   "source": [
    "**1) División de los datos**\n",
    "\n",
    "El algoritmo comienza con todos los datos en un nodo raíz y busca la característica (o variable) que mejor divide los datos en subconjuntos más homogéneos en términos de la variable de respuesta (el valor que estamos tratando de predecir)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe11002",
   "metadata": {},
   "source": [
    "**2) Criterio de división**\n",
    "\n",
    "Para determinar la mejor característica de división, se utiliza un criterio de impureza como el error cuadrático medio (MSE), que mide la dispersión de los valores de la variable de respuesta en un nodo. Se calcula la reducción en la impureza (o la ganancia en la pureza) que se obtiene al dividir los datos según una característica específica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade785a",
   "metadata": {},
   "source": [
    "**3) División recursiva**\n",
    "\n",
    "Una vez que se selecciona la mejor característica de división, los datos se dividen en dos o más subconjuntos en función de los valores de esa característica. Este proceso se repite recursivamente en cada subconjunto hasta que se alcanza un criterio de parada, como una profundidad máxima del árbol, un número mínimo de muestras en cada nodo hoja o una impureza mínima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457af500",
   "metadata": {},
   "source": [
    "**4) Predicción**\n",
    "\n",
    "Una vez construido el árbol, para hacer una predicción para una nueva observación, se sigue el árbol desde la raíz hasta llegar a una hoja, y se predice el valor promedio de las observaciones de entrenamiento en esa hoja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f6232",
   "metadata": {},
   "source": [
    "**5) Regularización**\n",
    "\n",
    "Para evitar el sobreajuste (overfitting), se pueden aplicar técnicas de regularización como la poda del árbol (eliminación de nodos innecesarios), la limitación de la profundidad máxima del árbol o la especificación de un número mínimo de muestras requeridas para dividir un nodo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4275b1",
   "metadata": {},
   "source": [
    "**Pregunta 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d770d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desempeño de los modelos:\n",
      "------------------------------------\n",
      "Boosting RMSE: 53.837130987019755\n",
      "Bagging RMSE: 57.06979759691468\n",
      "Random Forest RMSE: 54.332408273184846\n",
      "Regresión Lineal RMSE: 53.85344583676593\n",
      "Regresión Logística Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Cargar conjunto de datos de diabetes\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar los datos (solo para modelos lineales)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Inicializar modelos\n",
    "boosting_model = GradientBoostingRegressor(random_state=42)\n",
    "bagging_model = BaggingRegressor(random_state=42)\n",
    "random_forest_model = RandomForestRegressor(random_state=42)\n",
    "linear_regression_model = LinearRegression()\n",
    "logistic_regression_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Entrenar modelos\n",
    "boosting_model.fit(X_train, y_train)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "linear_regression_model.fit(X_train_scaled, y_train)\n",
    "logistic_regression_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluar modelos\n",
    "boosting_pred = boosting_model.predict(X_test)\n",
    "bagging_pred = bagging_model.predict(X_test)\n",
    "random_forest_pred = random_forest_model.predict(X_test)\n",
    "linear_regression_pred = linear_regression_model.predict(X_test_scaled)\n",
    "logistic_regression_pred = logistic_regression_model.predict(X_test_scaled)\n",
    "\n",
    "# Calcular métricas de desempeño\n",
    "boosting_rmse = np.sqrt(mean_squared_error(y_test, boosting_pred))\n",
    "bagging_rmse = np.sqrt(mean_squared_error(y_test, bagging_pred))\n",
    "random_forest_rmse = np.sqrt(mean_squared_error(y_test, random_forest_pred))\n",
    "linear_regression_rmse = np.sqrt(mean_squared_error(y_test, linear_regression_pred))\n",
    "logistic_regression_accuracy = accuracy_score(y_test, logistic_regression_pred)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Desempeño de los modelos:\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"Boosting RMSE: {boosting_rmse}\")\n",
    "print(f\"Bagging RMSE: {bagging_rmse}\")\n",
    "print(f\"Random Forest RMSE: {random_forest_rmse}\")\n",
    "print(f\"Regresión Lineal RMSE: {linear_regression_rmse}\")\n",
    "print(f\"Regresión Logística Accuracy: {logistic_regression_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078e55ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SERGHI\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "------------------------------------\n",
      "{'Model': 'Boosting', 'MSE': 2836.864, 'R Squared': 0.465, 'Best Params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}}\n",
      "{'Model': 'Bagging', 'MSE': 2810.76, 'R Squared': 0.469, 'Best Params': {'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100}}\n",
      "{'Model': 'Random Forest', 'MSE': 2869.245, 'R Squared': 0.458, 'Best Params': {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 150}}\n",
      "{'Model': 'Linear Regression', 'MSE': 2900.194, 'R Squared': 0.453, 'Best Params': {}}\n",
      "{'Model': 'Logistic Regression', 'MSE': 5705.472, 'R Squared': 0.0, 'Best Params': {'C': 10}}\n"
     ]
    }
   ],
   "source": [
    "# Importar GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir lista de modelos y sus hiperparámetros\n",
    "models = [\n",
    "    {\n",
    "        'name': 'Boosting',\n",
    "        'model': GradientBoostingRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'learning_rate': [0.01, 0.1, 0.5],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Bagging',\n",
    "        'model': BaggingRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [10, 50, 100],\n",
    "            'max_samples': [0.5, 1.0],\n",
    "            'max_features': [0.5, 1.0]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'model': RandomForestRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'max_depth': [None, 5, 10],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Linear Regression',\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Logistic Regression',\n",
    "        'model': LogisticRegression(max_iter=1000),\n",
    "        'params': {'C': [0.1, 1, 10]}\n",
    "    }\n",
    "]\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over models\n",
    "for model_info in models:\n",
    "    model_name = model_info['name']\n",
    "    model = model_info['model']\n",
    "    params = model_info['params']\n",
    "\n",
    "    # Apply GridSearchCV\n",
    "    grid = GridSearchCV(model, params, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = best_model.score(X_test, y_test)\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'MSE': round(mse, 3),\n",
    "        'R Squared': round(r2, 3),\n",
    "        'Best Params': grid.best_params_\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "print(\"Results:\")\n",
    "print(\"------------------------------------\")\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61c50701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "                 Model       MSE  R Squared\n",
      "0              Bagging  2810.760      0.469\n",
      "1             Boosting  2836.864      0.465\n",
      "2        Random Forest  2869.245      0.458\n",
      "3    Linear Regression  2900.194      0.453\n",
      "4  Logistic Regression  5705.472      0.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "df_results = pd.DataFrame(results)\n",
    "# Ordenar por R cuadrado de manera descendente\n",
    "df_results = df_results.sort_values(by='R Squared', ascending=False)\n",
    "# Restablecer el índice\n",
    "df_results = df_results.reset_index(drop=True)\n",
    "# Eliminar la columna 'Best Params'\n",
    "df_results.drop(columns='Best Params', inplace=True)\n",
    "\n",
    "# Imprimir DataFrame de resultados\n",
    "print(\"Results:\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67668248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desempeño de los modelos:\n",
      "                 Model      Score\n",
      "0             Boosting  53.837131\n",
      "1              Bagging  57.069798\n",
      "2        Random Forest  54.332408\n",
      "3    Linear Regression  53.853446\n",
      "4  Logistic Regression   0.000000\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Cargar conjunto de datos de diabetes\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar los datos (solo para modelos lineales)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Inicializar modelos\n",
    "models = [\n",
    "    ('Boosting', GradientBoostingRegressor(random_state=42)),\n",
    "    ('Bagging', BaggingRegressor(random_state=42)),\n",
    "    ('Random Forest', RandomForestRegressor(random_state=42)),\n",
    "    ('Linear Regression', LinearRegression()),\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000))\n",
    "]\n",
    "\n",
    "# Almacenar resultados en un diccionario\n",
    "results = []\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    if name == 'Logistic Regression':\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results.append({'Model': name, 'Score': accuracy})\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        results.append({'Model': name, 'Score': rmse})\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Imprimir DataFrame de resultados\n",
    "print(\"Desempeño de los modelos:\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "453e0ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desempeño de los modelos:\n",
      "                 Model       RMSE  Accuracy  R Squared\n",
      "0             Boosting  53.837131       NaN   0.452934\n",
      "1              Bagging  57.069798       NaN   0.385265\n",
      "2        Random Forest  54.332408       NaN   0.442823\n",
      "3    Linear Regression  53.853446       NaN   0.452603\n",
      "4  Logistic Regression        NaN       0.0        NaN\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, r2_score\n",
    "\n",
    "# Cargar conjunto de datos de diabetes\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar los datos (solo para modelos lineales)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Inicializar modelos\n",
    "models = [\n",
    "    ('Boosting', GradientBoostingRegressor(random_state=42)),\n",
    "    ('Bagging', BaggingRegressor(random_state=42)),\n",
    "    ('Random Forest', RandomForestRegressor(random_state=42)),\n",
    "    ('Linear Regression', LinearRegression()),\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000))\n",
    "]\n",
    "\n",
    "# Almacenar resultados en un diccionario\n",
    "results = []\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    if name == 'Logistic Regression':\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results.append({'Model': name, 'RMSE': np.nan, 'Accuracy': accuracy, 'R Squared': np.nan})\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        results.append({'Model': name, 'RMSE': rmse, 'Accuracy': np.nan, 'R Squared': r2})\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Imprimir DataFrame de resultados\n",
    "print(\"Desempeño de los modelos:\")\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
